grafana:
  adminPassword: adminadmin1
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"
    hosts:
      - grafana.joe-k8s-sandbox.adorsys-sandbox.aws.adorsys.de
    tls:
      - secretName: grafana-tls
        hosts:
          - grafana.joe-k8s-sandbox.adorsys-sandbox.aws.adorsys.de

alertmanager:
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"
      nginx.ingress.kubernetes.io/auth-type: "basic"
      nginx.ingress.kubernetes.io/auth-secret: "auth-basic"
      nginx.ingress.kubernetes.io/auth-secret-type: "auth-map"
      nginx.ingress.kubernetes.io/auth-realm: "Authentication Required - kibana"
    hosts:
      - alertmanager.joe-k8s-sandbox.adorsys-sandbox.aws.adorsys.de
    tls:
      - secretName: alertmanager-tls
        hosts:
          - alertmanager.joe-k8s-sandbox.adorsys-sandbox.aws.adorsys.de

kubeControllerManager:
  service:
    port: 10257 # https port
    targetPort: 10257
  serviceMonitor:
    https: true
    insecureSkipVerify: true
    serverName: 127.0.0.1

kubeScheduler:
  service:
    port: 10259
    targetPort: 10259
  serviceMonitor:
    https: true
    insecureSkipVerify: true
    serverName: 127.0.0.1

kubeEtcd:
  enabled: false
  service:
    port: 2379
    targetPort: 2379
    selector:
      component: etcd
  serviceMonitor:
    interval: ""
    scheme: https
    insecureSkipVerify: false
    serverName: "localhost"
    caFile: "/etc/kubernetes/pki/etcd/ca.crt"
    #certFile: "/etc/kubernetes/pki/etcd/healthcheck-client.crt"
    #keyFile: "/etc/kubernetes/pki/etcd/healthcheck-client.key"

prometheus:
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"
      nginx.ingress.kubernetes.io/auth-type: "basic"
      nginx.ingress.kubernetes.io/auth-secret: "auth-basic"
      nginx.ingress.kubernetes.io/auth-secret-type: "auth-map"
      nginx.ingress.kubernetes.io/auth-realm: "Authentication Required - kibana"
    hosts:
      - prometheus.joe-k8s-sandbox.adorsys-sandbox.aws.adorsys.de
    tls:
      - secretName: prometheus-tls
        hosts:
          - prometheus.joe-k8s-sandbox.adorsys-sandbox.aws.adorsys.de

  prometheusSpec:
    enableAdminAPI: true
    nodeSelector:
      node-role.kubernetes.io/master: ""
    tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule

    resources:
      requests:
        memory: 400Mi

    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 20Gi
    volumes:
      - name: etcd-ca
        hostPath:
          path: /etc/kubernetes/pki/etcd/ca.crt
      - name: etcd-healthcheck-client-crt
        hostPath:
          path: /etc/kubernetes/pki/etcd/healthcheck-client.crt
      - name: etcd-healthcheck-client-key
        hostPath:
          path: /etc/kubernetes/pki/etcd/healthcheck-client.key
    volumeMounts:
      - name: etcd-ca
        mountPath: /etc/kubernetes/pki/etcd/ca.crt
      - name: etcd-healthcheck-client-crt
        mountPath: /etc/kubernetes/pki/etcd/healthcheck-client.crt
      - name: etcd-healthcheck-client-key
        mountPath: /etc/kubernetes/pki/etcd/healthcheck-client.key

    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false
    additionalScrapeConfigs:
#      - job_name: 'kubernetes-apiservers'
#
#        kubernetes_sd_configs:
#          - role: endpoints
#
#        # Default to scraping over https. If required, just disable this or change to
#        # `http`.
#        scheme: https
#
#        # This TLS & bearer token file config is used to connect to the actual scrape
#        # endpoints for cluster components. This is separate to discovery auth
#        # configuration because discovery & scraping are two separate concerns in
#        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
#        # the cluster. Otherwise, more config options have to be provided within the
#        # <kubernetes_sd_config>.
#        tls_config:
#          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
#          # If your node certificates are self-signed or use a different CA to the
#          # master CA, then disable certificate verification below. Note that
#          # certificate verification is an integral part of a secure infrastructure
#          # so this should only be disabled in a controlled environment. You can
#          # disable certificate verification by uncommenting the line below.
#          #
#          insecure_skip_verify: true
#        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
#
#        # Keep only the default/kubernetes service endpoints for the https port. This
#        # will add targets for each API server which Kubernetes adds an endpoint to
#        # the default/kubernetes service.
#        relabel_configs:
#          - source_labels: [ __meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name ]
#            action: keep
#            regex: default;kubernetes;https
#
#      - job_name: 'kubernetes-nodes'
#
#        # Default to scraping over https. If required, just disable this or change to
#        # `http`.
#        scheme: https
#
#        # This TLS & bearer token file config is used to connect to the actual scrape
#        # endpoints for cluster components. This is separate to discovery auth
#        # configuration because discovery & scraping are two separate concerns in
#        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
#        # the cluster. Otherwise, more config options have to be provided within the
#        # <kubernetes_sd_config>.
#        tls_config:
#          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
#          # If your node certificates are self-signed or use a different CA to the
#          # master CA, then disable certificate verification below. Note that
#          # certificate verification is an integral part of a secure infrastructure
#          # so this should only be disabled in a controlled environment. You can
#          # disable certificate verification by uncommenting the line below.
#          #
#          insecure_skip_verify: true
#        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
#
#        kubernetes_sd_configs:
#          - role: node
#
#        relabel_configs:
#          - action: labelmap
#            regex: __meta_kubernetes_node_label_(.+)
#          - target_label: __address__
#            replacement: kubernetes.default.svc:443
#          - source_labels: [ __meta_kubernetes_node_name ]
#            regex: (.+)
#            target_label: __metrics_path__
#            replacement: /api/v1/nodes/$1/proxy/metrics
#
#
#      - job_name: 'kubernetes-nodes-cadvisor'
#
#        # Default to scraping over https. If required, just disable this or change to
#        # `http`.
#        scheme: https
#
#        # This TLS & bearer token file config is used to connect to the actual scrape
#        # endpoints for cluster components. This is separate to discovery auth
#        # configuration because discovery & scraping are two separate concerns in
#        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
#        # the cluster. Otherwise, more config options have to be provided within the
#        # <kubernetes_sd_config>.
#        tls_config:
#          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
#          # If your node certificates are self-signed or use a different CA to the
#          # master CA, then disable certificate verification below. Note that
#          # certificate verification is an integral part of a secure infrastructure
#          # so this should only be disabled in a controlled environment. You can
#          # disable certificate verification by uncommenting the line below.
#          #
#          insecure_skip_verify: true
#        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
#
#        kubernetes_sd_configs:
#          - role: node
#
#        # This configuration will work only on kubelet 1.7.3+
#        # As the scrape endpoints for cAdvisor have changed
#        # if you are using older version you need to change the replacement to
#        # replacement: /api/v1/nodes/$1:4194/proxy/metrics
#        # more info here https://github.com/coreos/prometheus-operator/issues/633
#        relabel_configs:
#          - action: labelmap
#            regex: __meta_kubernetes_node_label_(.+)
#          - target_label: __address__
#            replacement: kubernetes.default.svc:443
#          - source_labels: [ __meta_kubernetes_node_name ]
#            regex: (.+)
#            target_label: __metrics_path__
#            replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor

      # Scrape config for service endpoints.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: If the metrics are exposed on a different port to the
      # service then set this appropriately.
      - job_name: 'kubernetes-service-endpoints'

        kubernetes_sd_configs:
          - role: endpoints

        relabel_configs:
          - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_scrape ]
            action: keep
            regex: true
          - source_labels: [ __meta_kubernetes_namespace ]
            regex: monitoring|kube-system
            action: drop
          - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_scheme ]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_path ]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [ __address__, __meta_kubernetes_service_annotation_prometheus_io_port ]
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [ __meta_kubernetes_namespace ]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [ __meta_kubernetes_service_name ]
            action: replace
            target_label: kubernetes_name
          - source_labels: [ __meta_kubernetes_pod_node_name ]
            action: replace
            target_label: kubernetes_node

      # Scrape config for slow service endpoints; same as above, but with a larger
      # timeout and a larger interval
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: If the metrics are exposed on a different port to the
      # service then set this appropriately.
      - job_name: 'kubernetes-service-endpoints-slow'

        scrape_interval: 5m
        scrape_timeout: 30s

        kubernetes_sd_configs:
          - role: endpoints

        relabel_configs:
          - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_scrape_slow ]
            action: keep
            regex: true
          - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_scheme ]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_path ]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [ __address__, __meta_kubernetes_service_annotation_prometheus_io_port ]
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [ __meta_kubernetes_namespace ]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [ __meta_kubernetes_service_name ]
            action: replace
            target_label: kubernetes_name
          - source_labels: [ __meta_kubernetes_pod_node_name ]
            action: replace
            target_label: kubernetes_node

      - job_name: 'prometheus-pushgateway'
        honor_labels: true

        kubernetes_sd_configs:
          - role: service

        relabel_configs:
          - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_probe ]
            action: keep
            regex: pushgateway

      # Example scrape config for probing services via the Blackbox Exporter.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/probe`: Only probe services that have a value of `true`
      - job_name: 'kubernetes-services'

        metrics_path: /probe
        params:
          module: [ http_2xx ]

        kubernetes_sd_configs:
          - role: service

        relabel_configs:
          - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_probe ]
            action: keep
            regex: true
          - source_labels: [ __address__ ]
            target_label: __param_target
          - target_label: __address__
            replacement: blackbox
          - source_labels: [ __param_target ]
            target_label: instance
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [ __meta_kubernetes_namespace ]
            target_label: kubernetes_namespace
          - source_labels: [ __meta_kubernetes_service_name ]
            target_label: kubernetes_name

      # Example scrape config for pods
      #
      # The relabeling allows the actual pod scrape endpoint to be configured via the
      # following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
      - job_name: 'kubernetes-pods'

        kubernetes_sd_configs:
          - role: pod

        relabel_configs:
          - source_labels: [ __meta_kubernetes_pod_annotation_prometheus_io_scrape ]
            action: keep
            regex: true
          - source_labels: [ __meta_kubernetes_namespace ]
            regex: monitoring|kube-system
            action: drop
          - source_labels: [ __meta_kubernetes_pod_annotation_prometheus_io_path ]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [ __address__, __meta_kubernetes_pod_annotation_prometheus_io_port ]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [ __meta_kubernetes_namespace ]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [ __meta_kubernetes_pod_name ]
            action: replace
            target_label: kubernetes_pod_name
          - source_labels: [ __meta_kubernetes_pod_phase ]
            regex: Pending|Succeeded|Failed
            action: drop

      # Example Scrape config for pods which should be scraped slower. An useful example
      # would be stackriver-exporter which queries an API on every scrape of the pod
      #
      # The relabeling allows the actual pod scrape endpoint to be configured via the
      # following annotations:
      #
      # * `prometheus.io/scrape-slow`: Only scrape pods that have a value of `true`
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
      - job_name: 'kubernetes-pods-slow'

        scrape_interval: 5m
        scrape_timeout: 30s

        kubernetes_sd_configs:
          - role: pod

        relabel_configs:
          - source_labels: [ __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow ]
            action: keep
            regex: true
          - source_labels: [ __meta_kubernetes_pod_annotation_prometheus_io_path ]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [ __address__, __meta_kubernetes_pod_annotation_prometheus_io_port ]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [ __meta_kubernetes_namespace ]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [ __meta_kubernetes_pod_name ]
            action: replace
            target_label: kubernetes_pod_name
          - source_labels: [ __meta_kubernetes_pod_phase ]
            regex: Pending|Succeeded|Failed
            action: drop
